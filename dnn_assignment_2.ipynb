{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0284MVa_Tf4D"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "### Ferenc HuszÃ¡r, Nic Lane and Neil Lawrence\n",
    "\n",
    "### 23rd February 2021\n",
    "\n",
    "Welcome to the second assignment for the Deep Neural Networks module. In this assignment you will explore some of the model architectures we talked about in the second half of lectures.\n",
    "\n",
    "There are 230 marks given in total for this second assessment ($\\approx 70\\%$ of the total of 330 marks for this course), broken into three groups:\n",
    "* (A) 75 marks for the guided questions on Residual Networks\n",
    "* (B) 75 marks for the guided questions on RNNs\n",
    "* (C) 80 marks for a mini-project of your choice\n",
    "\n",
    "You can choose whichever mini-project you want to attempt, we give you a range of options. If you would like to do more of these, that's fine, but will only mark one, whichever appears fist in your submitted notebook. These tasks are a bit more exploratoy, you don't have to go overboard on them. Equally, if you struggle to get them to work the way you wanted/expected, please document what you tried and what you learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05FWpdUHKWJX"
   },
   "source": [
    "# A: Residual Networks\n",
    "\n",
    "In these questions we will work with residual networks (ResNets). In order to save time, we won't be training these models ourselves. Pytorch provides pre-*trained* weights for a range of commonly used models in the `torchvision` package, including ResNets of various depth. Most of the pretrained ResNets available are slightly improved versions of those proposed in 'Deep Residual Learning for Image Recognition' by [He et al (2015)](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "Note: you can request colab to run a GPU by going to Runtime -> Change runtime type. However, since we are only going to work with single images, it is unlikely this will buy you significant speedup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHinV_sjktHR"
   },
   "source": [
    "## Setting up: Blas the border collie\n",
    "\n",
    "To start, we will look at whether ResNets of different length can correctly classify my friend's dog Blas as a Border Collie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t150JN-WTQp6",
    "outputId": "47173b5a-a468-4be5-9832-b5d04635d83d"
   },
   "outputs": [],
   "source": [
    "!wget https://caballerojose.com/images/blas.jpg -P blas_the_dog/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXI3qGx1kGZw"
   },
   "source": [
    "### Loading an image\n",
    "\n",
    "We will use the Python Imaging Library (PIL) to load the image. Colab then displays the image in-line if it is the output of the last line in the cell. Hello Blas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "s7gdWkK8jipP",
    "outputId": "22526be6-921c-41a0-d75c-53df2d01f621"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('blas_the_dog/test/blas.jpg')\n",
    "img.resize((800,535))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWjZvR5GkkzR"
   },
   "source": [
    "### Transformations\n",
    "\n",
    "ConvNets expect input images to be of a certain size, otherwise the dimensions of the tensors within the network don't work out. We have to resize this image so it is of size $224\\times 224$ as the network expects. Secondly, when training neural networks, we often normalise the inputs (substract the mean and divide by the standard deviation). As this model was trained on ImageNet, we have to use the mean and standard deviation from this dataset. Thankfully, we don't have to compute this, as it's included in the [pytorch documentation](https://pytorch.org/vision/0.8/models.html) (as well as elsewhere on the internet).\n",
    "\n",
    "We will use `transforms` from the `torchvision` module to apply these transformations to the image. We also convert from `PIL.Image` to `torch.Tensor` in the process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xCh9JlSnm0r"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]\n",
    "                                )])\n",
    "\n",
    "img_t = transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfSUkhvZoibX"
   },
   "source": [
    "We can display the transformed image by converting it back to `PIL.Image`. We can see that the image was downsized, cropped to it's centre square region, and the colours are messed up due to normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "2QtknH5ejpCB",
    "outputId": "bd3e1793-8e84-4dbb-f09f-2e2fd0e261f9"
   },
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0on6svx2pALv"
   },
   "source": [
    "### Loading  the model\n",
    "\n",
    "Now we are going to load the pre-trained 18-layer ResNet model from the `torchvision` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV2cst6qpS0P"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1UbzOgvJbG-"
   },
   "source": [
    "Let's see how the model classifies Blas, the border collie. For this, we have to turn the image tensor into a minibatch of 1 example, which practically means adding an extra dimension to the tensor. This can be done using the `unsqueeze` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0o4AhVho0cI",
    "outputId": "3d625606-a6fe-4445-d612-c5481847e6c2"
   },
   "outputs": [],
   "source": [
    "x = img_t.unsqueeze(0)\n",
    "y = resnet18(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtlSwoFHKBQb"
   },
   "source": [
    "The model's output is a vector of 1000 logits, one for each class in the ImageNet dataset. To interpret this, we have to know what the ImageNet classes are. Also, the logits themselves are less human-readable, so we are going to turn them into probabilities using a `softmax` function. This helper function below will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DtSpT-0KbNm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "imagenet_labels_url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "response = requests.get(imagenet_labels_url)\n",
    "imagenet_class_names = json.loads(response.text)\n",
    "\n",
    "def top_5_classes(y, class_names = None):\n",
    "  if class_names==None:\n",
    "    imagenet_labels_url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "    response = requests.get(imagenet_labels_url)\n",
    "    class_names = json.loads(response.text)\n",
    "  p = softmax(y[0,:], dim=0)\n",
    "  values, indices = p.topk(5)\n",
    "  return [(class_names[index], value) for index, value in zip(indices.detach().numpy(), values.detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXokAmhCK8vf",
    "outputId": "24d33a16-b1d9-4412-f5b7-e7897ac5abae"
   },
   "outputs": [],
   "source": [
    "top_5_classes(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni4T1RoMLG2k"
   },
   "source": [
    "Oops, the model doesn't seem to recognise Blas. It thinks Blas is a bucket, plunger or hook. This is because in order to use the pretrained models in pytorch, we have to set them to evaluation mode. When training deep neural networks, sometimes the networks work differently at training and test time. For example, when using a technique called dropout, random units in the network may be dropped at training time, but at test time, all units are used and their output is scaled. Similarly, when using batch normalisation, the network works differently in training time and at test time. To set models to evaluation mode we can use the `eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_9ksHR0LBpM",
    "outputId": "0baa34a0-0a48-4374-8279-b679d49d19b0"
   },
   "outputs": [],
   "source": [
    "resnet18.eval()\n",
    "y = resnet18(x)\n",
    "top_5_classes(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COoqjfzELzqd"
   },
   "source": [
    "OK, getting there. At least the model now recognises Blas as a dog, but misses the specific breed. The correct class would be a Border Collie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E77NGf4MPdI"
   },
   "source": [
    "## Question A.1: Trying different pretrained networks\n",
    "\n",
    "Now try different models from `torchvision` and see which ones can classify Blas correctly. In particular, check out deeper versions of ResNets. Use the [documentation](https://pytorch.org/vision/0.8/models.html) to find out what other networks are available. Use the code blocks below, and the text field to summarise what you found. (For full marks, try multiple ResNets, and at least two different non-ResNets).\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvYETBgyNW5R"
   },
   "source": [
    "### Answer A.1\n",
    "\n",
    "Please use this cell to summarise which networks you tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-lHM-EqO3mo"
   },
   "outputs": [],
   "source": [
    "# please add code for question A.1 here. You may add further code cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wngMaz-bNRfm"
   },
   "source": [
    "## Inspecting and modifying networks\n",
    "\n",
    "For the next question, you will need to modify the pre-trained networks, and you will need to look inside them. Below, we share a few things that will help you learn how models work. First, let's look at what the `resnet18` architecture looks like and how that architecture is represented in pytorch. You can print the model to see a text description of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlkQVrpnOAqP",
    "outputId": "8ffa2743-1702-4bf7-9f56-316b8de3231f"
   },
   "outputs": [],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-ti6v84OnqU"
   },
   "source": [
    "Here, modules are listed from top to bottom in order of how they are executed.In a pytorch ResNet, residual blocks are organised into `layers`, each of which contain sequences of residual blocks. You will notice that:\n",
    "* the number of channels keeps doubling at each layer, in this case from 128 -> 256 -> 512.\n",
    "* at the same time, the figure size is halved with each layer. The image size is halved by `MaxPool2d` layers as well as strided convolutions where the stride is `2`.\n",
    "* a `resnet18` is built out of `BasicBlock` building blocks, which are the simple residual block proposed by [He et al, (2015)](https://arxiv.org/abs/1512.03385) which I showed in the lecture. Deeper ResNets, such as `resnet50` use a more complicated `Bottleneck` component instead.\n",
    "* in pytorch, the linear and nonlinear parts of a layer are separate, i.e. the convolution and subsequent ReLU activation are handled as separate layers.\n",
    "\n",
    "To access the second convolution layer from the second residual block at the third layer, you can write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brtTQ671Rroa",
    "outputId": "3493514c-da74-41a0-8d40-05bba86a8551"
   },
   "outputs": [],
   "source": [
    "resnet18.layer3[1].conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw_azspgPUIV"
   },
   "source": [
    "And you can access parameters of layers like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bmoxs_theo7M",
    "outputId": "0851f3e7-2772-4fc2-8b6b-1e15d1225ba1"
   },
   "outputs": [],
   "source": [
    "resnet18.conv1.weight.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f96bdrHDhbaW"
   },
   "source": [
    "We can visualise the convolution kernels on the first layer using the code below. In addition to showing how to access weights, you may find some of this code useful as an example of using the `matplotlib.pylab` package for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "uUSC7Dcue5GC",
    "outputId": "23143f4a-b64e-47de-f266-d663e3fe7862"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "f = plt.figure(figsize = (8,8))\n",
    "\n",
    "weights = resnet18.conv1.weight.detach().numpy()\n",
    "#normalising weights so a value of 0 becomes gray, and colours stay within [0,1]\n",
    "weights = weights/(np.abs(weights).max())/2+0.5\n",
    "\n",
    "#there are 64 convolution kernels, which we will arrange along an 8x8 grid\n",
    "for i, w in enumerate(weights):\n",
    "  plt.subplot(8, 8, i+1)\n",
    "  # we move the colour channel axis to the end, this is where imshow expects it\n",
    "  plt.imshow(np.moveaxis(w, 0, -1));\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YLxZNRkVIwI"
   },
   "source": [
    "In pytorch, you can inspect what each layer of the network does by using forward hooks. A forward hook is a python function that gets called whenever the layer is evaluated (i.e. data is passed through the layer). The forward hook function receives both the input and the output of the module. In pytorch, you can add a forward hook using the register_forward_hook function. \n",
    "\n",
    "For example, in the code below, I attach a hook to a layer, which simply prints something out. Instead of attaching the hook to the resnet18, I first create a `deepcopy` of the model, so the hook gets attached to the copy, but not to the `resnet18` model which I may use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGQv3pPEVZ0u",
    "outputId": "021933c5-9775-4e5e-ede5-54320c93bb22"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def useless_hook(module, input, output):\n",
    "  print('The hook was called.')\n",
    "\n",
    "model = copy.deepcopy(resnet18)\n",
    "model.layer3[1].conv2.register_forward_hook(useless_hook)\n",
    "y = model(x)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRiQssMLNMjM"
   },
   "source": [
    "## Question A.2: Inspecting layers in neural networks\n",
    "\n",
    "This question has a subtasks:\n",
    "1. print the number of parameters in each of the operations/layers of the `ResNet18`, and count how many there are in total.\n",
    "1. print the *shape of the output tensor* of the second convolution operation (`conv2`) within every single residual block in a `ResNet18`. *Tip:* you can use a forward hook.\n",
    "1. Extract the activation (output) of a layer of your choice within the ResNet18, and visualise the activations using `matplotlib.pylab.imshow`. *Tip:* you can use a forward for this here, too.\n",
    "1. Visualise the convolution weights (also called kernels) in the first convolution layer of a pretrained AlexNet and a VGG19 network. Compare it to what we obtained the ResNet above. Document the similarities and differences do you notice.\n",
    "\n",
    "\n",
    "*Tips:*\n",
    "* you can use `copy.deepcopy` to create copies of models.\n",
    "* You can use `tensor.size()` to get the sape.\n",
    "* you have to use `tensor.detach().numpy()` to detach a pytorch tensor from the computational graph and convert it to a numpy array.\n",
    "* for plotting, you can use the `matplotlib.pylab` module. The function `imshow` can be usedd to plot activation maps as images, `axis('off')` is handy for removing axis labels. `subplot` allows you to create grids of multiple images in one figure, finally `figure(figsize=(x, y))` lets you change the overall size of the figure.\n",
    "* on a pytorch module you can call `named_children` and `name_parameters` to iterate over layers and parameters, respectively.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9gCHg2FXHFP"
   },
   "source": [
    "### Answer A.2\n",
    "\n",
    "please provide your answers in code cells below (you may use multiple code cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg9TIoAYP2kU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHoK_XnLnXly"
   },
   "source": [
    "## Editing models\n",
    "\n",
    "Models can be edited rather flexibly in pytorch. Underlying a pytorch `Module` are python `OrderedDict` objects. These are ordered lists of key-value pairs. Each submodule and each parameter has a name, which is the key. Members of the dictionay can be accessed as attributes of the `Module`. Adding a new parameter or layer to a module will add a new key-value pair to this dictionary.\n",
    "\n",
    "Let's try adding a new operation to our resnet18, which converts the logits to probabilities. We will do this by replacing the final operation by a `Sequential` module, which chains multiple operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6t1-S6nnZyw"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Softmax, Sequential\n",
    "\n",
    "model = copy.deepcopy(resnet18)\n",
    "model.fc = Sequential(\n",
    "    model.fc,\n",
    "    Softmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6kY3L_H4Q9u",
    "outputId": "76146d49-c765-4489-dc80-ff96a35233fc"
   },
   "outputs": [],
   "source": [
    "#let's see what the model looks like now\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J74UJ1EdHUa8"
   },
   "source": [
    "Let's check the model still works and that it outputs a vector of probabilities which therefore sums to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7GbZI7V25IR",
    "outputId": "75bbdc32-d6ca-4187-ff93-0accc56c9f82"
   },
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "print(y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxjTTC1D6rx0"
   },
   "source": [
    "We can also replace existing layers with other layers. Here, for example, I replace the first convolution layer with a randomly initialized one of the same dimensions. We can check what effect this has on the classification of Blas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wT3qQA0c5Uwi",
    "outputId": "e89a1a84-a5af-46cf-ff72-138799ec3268"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "model = copy.deepcopy(resnet18)\n",
    "model.conv1 = Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "y = model(x)\n",
    "top_5_classes(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ2qnL7k75ld"
   },
   "source": [
    "Or, if we replace the convolutional layer by the convolutions from the VGG19 network, followed by a MaxPooling, to account for the difference in stride, the network becomes pretty certain that Blas is, in fact, spider web, or possibly a type of spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "9bca8f2e8bbb462191b0af4f84c0783d",
      "3d259b0d17cf4372b743bafb54ce7558",
      "c1d7f0b1b97348ab802220d08339388d",
      "bb63d9b71fc040a2820d60eb8a100a34",
      "1fc73e90e9584f869450f517def60d16",
      "ea00f36b5945488b9ff1c07fd5df2460",
      "a262986b65094e299f23b461dd697ef0",
      "ac8d7d91d12a438da6c0419ca8d2a52f"
     ]
    },
    "id": "G8DuCOqS6_TJ",
    "outputId": "7430f2ce-c7a0-4b07-d3cf-acd648d6a24b"
   },
   "outputs": [],
   "source": [
    "from torch.nn import MaxPool2d\n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "model = copy.deepcopy(resnet18)\n",
    "model.conv1 = Sequential(\n",
    "      vgg19.features[0],\n",
    "      MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False))\n",
    "y=model(x)\n",
    "top_5_classes(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvVHVHAJ5Q55"
   },
   "source": [
    "## Question A.3: Dropping layers in ResNet15\n",
    "\n",
    "This question is composed of several tasks:\n",
    "1. Delete the second `BasicBlock` from `layer4` of the ResNet18, and see how the network's predictions change. \n",
    "1. Delete the first `BasicBlock` from `layer4` of the ResNet18, and see how the network's predictions change. (Note, this is a bit morer involved than the previous one).\n",
    "1. Delete the first `BasicBlock` from `layer2` of the ResNet18, and see how the network's predictions change.\n",
    "\n",
    "Tips:\n",
    "* There are several ways for removing a ResNet block:\n",
    "  * The easiest perhaps is to replace the entire block with a `torch.nn.Identity`. This works for blocks where the output and input featuremap size are the same. However, some of the residual blocks also reduce the size of the image (strided convolutions, or Max Pooling), so replacing by the Identity won't work. This is why the second task is a bit harder than the first. To find out what you have to do in the second task, look closely at the difference between the two `BasicBlock` instances within `layer4`. To understand how this block works, you can also look at the [source code](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L67).\n",
    "  * Since the activations are all ReLU, you can also ignore the effect of layers by setting appropriate weights and biases to zero. This is because $\\operatorname{ReLU}(0z).= 0$. However, if you are trying to use this method, be aware of batch normalization layers - you also want to remove the effect of those.\n",
    "  * You will notice that the layers of ResNets are of `torch.nn.Sequential` type. You can drop an element from this by creating a new `Sequential` instead with one operation removed.\n",
    "* You can solve this task using a few lines of code only.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNfY7MfGW5eh"
   },
   "source": [
    "### Answers A.3\n",
    "\n",
    "Please provide your answer in the code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVt-7Y4Z8j95"
   },
   "outputs": [],
   "source": [
    "#1. dropping layer4 second block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxxndArlXeS9"
   },
   "outputs": [],
   "source": [
    "#2. dropping layer4 first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD8FWc3jyAFo"
   },
   "outputs": [],
   "source": [
    "#2. dropping layer2 first block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF3G5oeDI0lr"
   },
   "source": [
    "## A convenience function: flattening ResNets\n",
    "\n",
    "Residual blocks in pytorch ResNets are organised into 4 layers. I found this rather cumbersome for the next exercise we are going to do. In order to make ResNets easier to work with I created the convenience function below which can *flatten* the layer hierarchy of pytorch resnet models, merging the 4 layers into one, called `residual_blocks`. After flattening, instead of accessing `model.layer1[0]` you can do `model.residual_blocks[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScqkMXv1lwFw"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def flatten_resnet(resnet_model):\n",
    "  \"\"\"\n",
    "  Takes a pytorch resnet model and returns a copy of the same model in a \n",
    "  flattened format, where the residulal blocks are not divided into four layers\n",
    "  but are instead all in a residual blocks component.\n",
    "  \"\"\"\n",
    "  model = copy.deepcopy(resnet_model)\n",
    "  residual_blocks = nn.Sequential(*model.layer1,\n",
    "                *model.layer2,\n",
    "                *model.layer3,\n",
    "                *model.layer4)\n",
    "  return nn.Sequential(OrderedDict([\n",
    "     ('conv1', model.conv1),\n",
    "     ('bn1', model.bn1),\n",
    "     ('relu', model.relu),\n",
    "     ('maxpool', model.maxpool),\n",
    "     ('residual_blocks', residual_blocks),\n",
    "     ('avgpool', model.avgpool),\n",
    "     ('flatten', nn.Flatten()),\n",
    "     ('fc', model.fc)\n",
    "  ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtDchsshdSL"
   },
   "source": [
    "## Question A.4: Dropping layers in ResNet50\n",
    "\n",
    "Load a pretrained ResNet50, and evaluate the model's predictions on the photo of Blas. Does it get the breed correct? (Border Collie).\n",
    "\n",
    "Look at the structure of the ResNet50. Identify the `Bottleneck` blocks which are used instead of `BasicBlock` building blocks in deeper ResNets. These are still arranged into 4 'layers', and there are 16 of them in total.\n",
    "\n",
    "Drop each of the 16 `Bottleneck` blocks from the ResNet50 (one at a time, not all at once), and make predictions with each of the perturbed networks. Plot the predicted probability of the Border Collie class (ImageNet class number 232) as a function of the index of the layer dropped. Compare the probabilities you get with the probability for the unperturbed model. What do you see?\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2nZuSNvjAJY"
   },
   "source": [
    "### Answers A.4\n",
    "\n",
    "Please add text in this box, and use as many code cells below as you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-vuiiZPjZm8"
   },
   "source": [
    "## Question A.5: Permuting and Repeating Layers\n",
    "\n",
    "Now, let's see what happens if we permute or repeat layers in the ResNet.\n",
    "\n",
    "1. Permute layers: Swap `resnet50.layer2[1]` and `resnet50.layer2[3]` in the network (so that instead of the second `Bottleneck` unit in `layer2` you use the fourth and vice versa). Make predictions with the modified network. How did perturbing the model this way modify the predictions?\n",
    "1. Repeat a residual block in the ResNet. Apply the second `Bottleneck` block of `layer2` of the ResNet twice instead of just once, making the model deeper. Make predictions with the modified model. What do you find?\n",
    "1. Now repeat the layer 2, 3, 4, 5 times. Do the predictions keep improving?\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHwJrB17Aprs"
   },
   "source": [
    "### Answers A.5\n",
    "\n",
    "Please add text in this box, and use as many code cells below as you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvNqkdKCmyX0"
   },
   "source": [
    "# B: Recurrent Networks\n",
    "\n",
    "In this part of the assignment, we will train simple recurrent networks to detect messages hidden in binary strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uwv2662c5h4"
   },
   "source": [
    "## Backgroud\n",
    "\n",
    "### Creating the dataset\n",
    "\n",
    "Below, I included a function which creates a pytorch `Dataset` we will use in the questions below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0BhhSIam0Sr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_binary_dataset(num_datapoints, signature_length=20, random_length=20):\n",
    "  '''\n",
    "  Creates a labelled dataset of `num_datapoints` binary sequences. Each sequence\n",
    "  is of total length `signature_length + random_length`. Positive examples have\n",
    "  a signature string of length `signature_length` embedded starting at a random\n",
    "  location within the string. The signature is a binary string of alternating 1s\n",
    "  and 0s. Negative examples are random bitstrings. They may contain the\n",
    "  signature by chance, although the probability of this diminishes as the\n",
    "  `signature_legth` increases.\n",
    "\n",
    "  The returned dataset contains positive examples first, followed by negatives.\n",
    "  When using minibatch-SGD, the dataset has to be shuffled before broken into\n",
    "  minibatches.\n",
    " \n",
    "  Attributes\n",
    "  ----------\n",
    "  num_datapoints : int\n",
    "    Number of datapoints to be generated. Half of the datapoints will be\n",
    "    positive, the rest negative. If `num_datapoints==1` then the dataset will\n",
    "    contain a single positive example.\n",
    "  signature_length : int\n",
    "    Length of the signature string\n",
    "  random_length : int\n",
    "    Number of random bits in the sequence in addition to the signature sequence.\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  torch.utils.data.TensorDataset\n",
    "    Pytorch dataset containing the strings and their binary labels.\n",
    "  '''\n",
    "  num_positives = num_datapoints - num_datapoints // 2\n",
    "  y = np.array([[1.0]]*num_positives + [[0.0]]*(num_datapoints - num_positives))\n",
    "  X = np.random.randint(2, size=(num_datapoints, signature_length + random_length))\n",
    "  signature = np.array((signature_length//2)*[1.0, 0.0])\n",
    "  for i in range(num_positives):\n",
    "    j = np.random.randint(random_length)\n",
    "    X[i, j:(j+signature_length)] = signature\n",
    "  X = torch.tensor(X, dtype=torch.float)[:, :, None]\n",
    "  y = torch.tensor(y)\n",
    "  return TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6rea8zXcz_t"
   },
   "source": [
    "Below, I visualise a dataset of $100$ positive and $100$ negative sequences of lenth $40$ each. In the visualisation below, positive examples appear on top, negatives in the bottom half. Black pixels show $1$s, white pixels show $0$s. You can see the signature sequence `10101010101010101010` embedded in the positive examples at random locations.\n",
    "\n",
    "*Notes:* You can see how I used `spy` to visualise a sparse matrix. This function can be used to show non-zero values in a matrix. The pytorch `TensorsDataset` object has a `tensors` attribute which contains the input and output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "3kG51-C2qJrB",
    "outputId": "b0b6e2a8-560f-4306-dc44-5e4b5a9b8d2d"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "ds = get_binary_dataset(200,\n",
    "                        signature_length=20,\n",
    "                        random_length=20)\n",
    "\n",
    "plt.figure(figsize=(4, 16))\n",
    "plt.spy(ds.tensors[0][:, :, 0].numpy())\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyE3cB1ZegJi"
   },
   "source": [
    "## Building a classifier\n",
    "\n",
    "Now we're going to build a simple RNN-based classifier for classifying binary sequences. We are going to feed the sequence into an RNN, then extract the hidden state activations at the end of the sequence, finally apply a linear transformation to calculate the classifier output.\n",
    "\n",
    "An RNN module returns two outputs, as explained in [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). We have to select a layer which selects the second output - which is the activation of the hidden state at the end of the sequence - rearranges and flattens it so it's ready to be fed into a linear layer.\n",
    "\n",
    "*Note:* By applying a further `sigmoid`, this classifier output can be turned into a probability, like it was the case with `softmax` the ResNet example. We are not going to make the sigmoid part of the network, as training will be more numerically stable this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZR55uqkdNG_"
   },
   "outputs": [],
   "source": [
    "class extract_last_cell(nn.Module):\n",
    "  '''Pytorch module which extracts and flattens the last hidden state of an RNN.\n",
    "  '''\n",
    "  def forward(self,x):\n",
    "    _ , out = x\n",
    "    return out.permute(1,0,2).flatten(start_dim=1)\n",
    "\n",
    "def get_basic_RNN_classifier():\n",
    "  return nn.Sequential(\n",
    "    nn.RNN(input_size=1, hidden_size=20, num_layers=2, batch_first=True),\n",
    "    extract_last_cell(),\n",
    "    nn.Linear(in_features = 20*2, out_features=1)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQTbded3gM5x"
   },
   "source": [
    "## Question B.1: Evaluating the loss\n",
    "\n",
    "Evaluate the log-loss of a randomly initialized classifier on minibatches of training examples from the dataset we defined before.\n",
    "\n",
    "*Tips:*\n",
    "* you can use [`pytorch/utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to sample minibatches of training data from the dataset\n",
    "* or, you can also access the input and output tensors of a `TensorDataset` directly via the `tensors` attribute\n",
    "* use [`torch.nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) or [`torch.nn.functional.binary_cross_entropy_with_logits`](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqLXCEQ3hOpQ"
   },
   "source": [
    "### Answer B.1\n",
    "\n",
    "pleasse add answer in the code cell(s) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_8wAi5sQoez"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IThrnbTQiTmm"
   },
   "source": [
    "## Training the classifier\n",
    "\n",
    "In the previous assignment we wrote a training loop to show how optimizers work in pytorch. For this assignment we are going to use ignite, a flexible module for reusable training loops in pytorch. We will install ignite, and then I provide a function you can use to train models on the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojgXMK9BieNL",
    "outputId": "0eb08ee3-4a35-4c5e-912f-6025c6e19dbf"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQepIAoyhtiL"
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "def fit_model_basic(model, training_dataset, test_dataset):\n",
    "  '''Fits a pytorch model to the training dataset using stochastic gradient\n",
    "  descent and tracks progress on a test set.\n",
    "\n",
    "  Arguments\n",
    "  ---------\n",
    "  model : pytorch.nn.Module\n",
    "    pytorch model that can be evaluated on input tensor in the dataset, and\n",
    "    returns a tensor that matches the shape of the output tensor.\n",
    "  training_dataset : pytorch.util.data.Dataset\n",
    "    used to train the model\n",
    "  test_dataset : pytorch.util.data.Dataset\n",
    "    used to evaluate the performance of the model at the end of each epoch\n",
    "\n",
    "  Retuns\n",
    "  ------\n",
    "  pytorch.nn.Module\n",
    "    the model after training\n",
    "  '''\n",
    "\n",
    "  train_loader = DataLoader(\n",
    "      training_dataset,\n",
    "      batch_size=100,\n",
    "      shuffle=True\n",
    "  )\n",
    "  val_loader = DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=1000,\n",
    "      shuffle=False,\n",
    "  )\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "\n",
    "  def thresholded_output_transform(output):\n",
    "      y_pred, y = output\n",
    "      y_pred = torch.heaviside(y_pred, values=torch.zeros(1))\n",
    "      return y_pred, y\n",
    "\n",
    "  val_metrics = {\n",
    "      \"accuracy\": Accuracy(thresholded_output_transform),\n",
    "      \"bce\": Loss(criterion)\n",
    "  }\n",
    "  evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
    "\n",
    "  @trainer.on(Events.ITERATION_COMPLETED(every=10))\n",
    "  def log_training_loss(trainer):\n",
    "      print(f\"Epoch[{trainer.state.epoch}] Loss: {trainer.state.output:.2f}\")\n",
    "\n",
    "  @trainer.on(Events.EPOCH_COMPLETED)\n",
    "  def log_training_results(trainer):\n",
    "      evaluator.run(train_loader)\n",
    "      metrics = evaluator.state.metrics\n",
    "      print(f\"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['bce']:.2f}\")\n",
    "\n",
    "  @trainer.on(Events.EPOCH_COMPLETED)\n",
    "  def log_validation_results(trainer):\n",
    "      evaluator.run(val_loader)\n",
    "      metrics = evaluator.state.metrics\n",
    "      print(f\"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['bce']:.2f}\")\n",
    "\n",
    "  trainer.run(train_loader, max_epochs=5)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHGyOammRKnq"
   },
   "source": [
    "The code above looks rather long and complicated, but that is because we ask ignite to log the progress of training in a very verbose way. Ignite uses python decorators (like the `@trainer.on(Events.EPOCH_COMPLETED)`) bits. You will find plenty of tutorials online if you google 'python decorators' if you want to learn more about them. In ignite, these decorators are used to register functions which are called when various events happen during training.\n",
    "\n",
    "For example, `@trainer.on(Events.ITERATION_COMPLETED(every=10))` tells ignite to execute the function that's below it every 10th iteration is completed. You will find that this training loop is largely a copy of the example code provided in the [ignite master documentation](https://pytorch.org/ignite/quickstart.html#code).\n",
    "\n",
    "One difference is that we use the `thresholded_output_transform` to tarnsform the network's output before [`Accuracy`](https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy) metric can be evaluated. Accuracy compares binary predictions to binary labels. But our network outputs `logits` instead of binary predictors. To turn these into binary decisions, we have to determine whether the logits are above or below $0$, which is what the `heaviside` function does.\n",
    "\n",
    "Ignite can do much more than what we ask it to do here. It's a flexible way of creating reproducible training loops for neural network training. It can handle distributed training over several machines, etc.\n",
    "\n",
    "Let's now use our training loop to train a classifier on the binary dataset we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypegwqc1iSxM",
    "outputId": "669f0038-456b-4f7e-e87f-40ab401648a7"
   },
   "outputs": [],
   "source": [
    "ds_train = get_binary_dataset(5000, signature_length=10, random_length=10)\n",
    "ds_test = get_binary_dataset(1000, signature_length=10, random_length=10)\n",
    "rnn_classifier = get_basic_RNN_classifier()\n",
    "\n",
    "fit_model_basic(rnn_classifier, ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s6VAmMtmz1S"
   },
   "source": [
    "Oh no! The error doesn't seem like the loss is reduced. Let's try to fix this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49QoLR7fnBLK"
   },
   "source": [
    "## Question B.2: Improving the SGD code\n",
    "\n",
    "1. Create a new version of `fit_model_basic` function, call it `fit_model`. You will use this function in subsequent questions. Your function should take additional arguments allowing you to easily specify key hyperparameters of optimization: learning rate, batchsize, whatever else you find important.\n",
    "1. Tune hyperparameters and change things until training works reliably. Consider [changing the optimizer](https://pytorch.org/docs/stable/optim.html) from vanilla SGD: you may try SGD with momentum, `Adam`, etc. You should be able to make this example work, i.e. reach $90\\%+$ average accuarcy within a few seconds of training.\n",
    "\n",
    "*Tips*:\n",
    " * You can try different learning rates, such as the [Karpathy constant](https://twitter.com/karpathy/status/801621764144971776). This is a joke, learning rates are usually problem-specific. Values between $0.1$ and $0.001$ are often tried.\n",
    " * If you want, you can run grid search: running training for different values of hyperparameters, and checkind which one produces the best results. If doing so, consider spacing parameter values logarithmically rather than linearly. I.e. it makes more sense to try learning rates $[0.001, 0.01, 0.1]$ than to try $[0.001, 0.002, 0.003, \\ldots]$.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8MvdOqhpVzQ"
   },
   "source": [
    "### Answer B.2\n",
    "\n",
    "Please modify the code in the cells below. You may use this text box to summarise the changes you made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJC-vK8ClY2Q"
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "def fit_model(model, training_dataset, test_dataset):\n",
    "  '''Fits a pytorch model to the training dataset using stochastic gradient\n",
    "  descent and tracks progress on a test set.\n",
    "\n",
    "  Arguments\n",
    "  ---------\n",
    "  model : pytorch.nn.Module\n",
    "    pytorch model that can be evaluated on input tensor in the dataset, and\n",
    "    returns a tensor that matches the shape of the output tensor.\n",
    "  training_dataset : pytorch.util.data.Dataset\n",
    "    used to train the model\n",
    "  test_dataset : pytorch.util.data.Dataset\n",
    "    used to evaluate the performance of the model at the end of each epoch\n",
    "\n",
    "  Retuns\n",
    "  ------\n",
    "  pytorch.nn.Module\n",
    "    the model after training\n",
    "  '''\n",
    "\n",
    "  train_loader = DataLoader(\n",
    "      training_dataset,\n",
    "      batch_size=100,\n",
    "      shuffle=True\n",
    "  )\n",
    "  val_loader = DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=1000,\n",
    "      shuffle=False,\n",
    "  )\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "\n",
    "  def thresholded_output_transform(output):\n",
    "      y_pred, y = output\n",
    "      y_pred = torch.heaviside(y_pred, values=torch.zeros(1))\n",
    "      return y_pred, y\n",
    "\n",
    "  val_metrics = {\n",
    "      \"accuracy\": Accuracy(thresholded_output_transform),\n",
    "      \"bce\": Loss(criterion)\n",
    "  }\n",
    "  evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
    "\n",
    "  @trainer.on(Events.ITERATION_COMPLETED(every=10))\n",
    "  def log_training_loss(trainer):\n",
    "      print(f\"Epoch[{trainer.state.epoch}] Loss: {trainer.state.output:.2f}\")\n",
    "\n",
    "  @trainer.on(Events.EPOCH_COMPLETED)\n",
    "  def log_training_results(trainer):\n",
    "      evaluator.run(train_loader)\n",
    "      metrics = evaluator.state.metrics\n",
    "      print(f\"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['bce']:.2f}\")\n",
    "\n",
    "  @trainer.on(Events.EPOCH_COMPLETED)\n",
    "  def log_validation_results(trainer):\n",
    "      evaluator.run(val_loader)\n",
    "      metrics = evaluator.state.metrics\n",
    "      print(f\"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['bce']:.2f}\")\n",
    "\n",
    "  trainer.run(train_loader, max_epochs=5)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxmUHtQVptgu"
   },
   "outputs": [],
   "source": [
    "classifier = fit_model(get_basic_RNN_classifier(), ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEambLYJqQFi"
   },
   "source": [
    "## Question B.3: Test on longer sequences\n",
    "\n",
    "Now that you successfully trained an RNN, let's test how well it works when we use it on longer sequences than what it was trained on. Does it have the ability to remember? Remember we trained on datasets with `signature_length=10, random_length=10`, that is a total length of `20` binary symbols.\n",
    "\n",
    "* create 5 test datasets with `signature_length` of $10$ but with different `random_length` parameters (e.g. $10$, $20$, $30$, $50$ and $100$)\n",
    "* evaluate accuracy of your trained model on each of these test regimes - (without retraining!)\n",
    "* plot the performance you achieve for different test sequence lengths\n",
    "* describe what you find\n",
    "\n",
    "*Tips:*\n",
    "* look inside the `fit_model` function for tips on evaluating models using ignite components.\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoVK-Wv0xZbI"
   },
   "source": [
    "### Answers B.3\n",
    "\n",
    "Please complete the task in code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDY_VmYgUB5O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ4Mr7KZxcsn"
   },
   "source": [
    "## Question B.4: Better models\n",
    "\n",
    "Remember what we learned about vanilla RNN's ability to remember information over long time range. Pick a different RNN cell type that should have better ability to remember, and define a new network that uses this cell type. \n",
    "* Modify the function below to define an improved RNN classifier\n",
    "* train an instance of this better RNN model on the same data we trained the previous classifier on\n",
    "* repeat the analysis from Question 3 using your new model, and contrast your new architecture's performance with the vanilla RNN from before.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77dInNw2BnjN"
   },
   "source": [
    "### Answers B.4\n",
    "\n",
    "Please edit the code below and add as many code cells as you would like to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INAFqA6Ex-FB"
   },
   "outputs": [],
   "source": [
    "def get_better_RNN_classifier():\n",
    "  '''A better RNN architecture for solving the signature detection problem.\n",
    "  '''\n",
    "  return nn.Sequential(\n",
    "    nn.RNN(input_size=1, hidden_size=20, num_layers=2, batch_first=True),\n",
    "    extract_last_cell(),\n",
    "    nn.Linear(in_features = 20*2, out_features=1)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50Rk0TvNyKtK"
   },
   "outputs": [],
   "source": [
    "classifier = fit_model(get_better_RNN_classifier(), ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udqTvmGcz8ab"
   },
   "source": [
    "## Question B.5: Visualise memorization\n",
    "\n",
    "Now we are going to visualize which parts of the input the trained architecture pays attention to when making its decision. This is going to be similar in spirit to [this distill post](https://distill.pub/2019/memorization-in-rnns/).\n",
    "* take your pre-trained RNN classifier from the previous example\n",
    "* generate a random positive input sequence (one that has a signature of length $10$ in it) and form an input tensor to the model.\n",
    "* calculate the gradient of the model's output with respect to the input.\n",
    "\n",
    "*Tips:*\n",
    "* remember how we used `requires_grad` and `backward` and the `grad` attribute in the lecture to calculate the gradients of an RNN's hidden state with respect to the input sequence.\n",
    "* remember the `TensorDataset` has a `tensors` argument which you can use to extract the input and output tensors from the dataset.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3zFVkO3_4QS"
   },
   "source": [
    "### Answers B.5\n",
    "\n",
    "Please add your solution to the code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX3eEkQJUjpK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxzDVXO_OAlx"
   },
   "source": [
    "## C Mini-Projects\n",
    "\n",
    "Please **choose one** of the proposed mini-project questions below. Add as many text and code cells under the heading as you'd like. You may attempt more than one, but we will only mark one: please write below which mini-project you'd like to get marked on:\n",
    "\n",
    "**I would like my solution for mini-project __________ to be marked.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDpoJPwkO35G"
   },
   "source": [
    "## C.1: Adversarial examples for ResNets\n",
    "\n",
    "Adversarial examples for image classifiers are images which have been imperceptibly changed such that the classifier's output changes. For example, below we see an image of a panda, which the network correctly classifies as panda (left), but when we add a small amount of structured noise (middle), and feed the modified image (right) into the classifier, it now classifies it is a gibbon with high confidence.\n",
    "\n",
    "![](https://openai.com/content/images/2017/02/adversarial_img_1.png)\n",
    "\n",
    "In this mini-project, your task is to generate an adversarial examples for a pretrained convolutional neural network of your choice (VGG19, ResNet18 or ResNet50 are good choices). Use the image of Blas as a starting point. Change the image of Blas imperceptibly so that your network misclassifies him as a bus or sea slug, or any other arbitrary ImageNet class which is clearly not a dog. I recommend using the *fast gradient sign* method described on page 3 of [(Szegedy et al, 2013)](https://arxiv.org/pdf/1412.6572.pdf). *(40 marks)*\n",
    "\n",
    "For full marks, explore the topic a bit more, example questions you can ask:\n",
    "* Can you deliberately control what class you get after the adversarial attack? *(~20 marks)*\n",
    "* Does the adversarial example generated for one network fool other networks, too? *(~20 marks for non-minimal investigation and good writeup)*\n",
    "* Load another image. Does the adversarial noise generated for one image work to fool the network when added to a different image *(~20 marks for for non-minimal investigation and good writeup)*\n",
    "* Can you generate adversarial examples without access to gradients, only using the forward pass of the network (this is called black-box adversarial attack)? *(~40 marks)*\n",
    "\n",
    "*Tips:*\n",
    "* The *fast gradient sign* method relies on gradients of the network's output with respect to pixel values in image space: you are looking for the smallest change to pixel values which maximally change the classifier's outputs. Remember how we did this using `requires_gradients` and `backward` in the RNN example.\n",
    "* To illustrate the change is imperceptible, you want to apply the change to the untransformed image, i.e. before the normalisation is applied. The normalization is itself differentiable, so my advice is to make the normalization part of the network itself.\n",
    "\n",
    "*80 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7-3qZ-VWV9S"
   },
   "source": [
    "## C.2: Extend on binary sequence learning from questions B\n",
    "\n",
    "Building on the quided question set B on Recurrent Neural Networks, explore what RNNs and different models can do on the binary string datasets. You can try the following\n",
    "* Make the task more difficult (for this, extend `get_binary_dataset`):\n",
    "  * Can you successfully train networks on longer training sequences? *(~10 marks)*\n",
    "  * What happens if you add noise to the data, e.g. randomly flip bits?  *(~20 marks)*\n",
    "  * Embed more than one copy of the signature sequence, can you train a network to count them?  *(~20 marks)*\n",
    "  * Can networks memorise messages emmbedded in random bit strings. For example, can you train a Seq2Seq network recover k random bits that immediately follow the signature sequence?  *(~40 marks)*\n",
    "* Try different architectures:\n",
    "  * What architectures, other than RNNs could solve this problem well? Do you need deep architectures? Design another model and train it on the binary dataset. Can you get them to solve the problem? Can you illustrate the pros and cons of different architectures for this problem? *(~30 marks for one architecture, up to ~70 marks for trying and comparing substantially different architectures in a good writeup)*\n",
    "* Hand design neural networks that solve this problem:\n",
    "  * This problem can be solved with simple algorithms. Implement these algorithms in neural hardware, i.e. by manually setting weights in a neural network. *(~40 marks)*\n",
    "\n",
    "*Tips:*\n",
    "  * For counting, you might want to think about what an appropriate loss function is. You can set this up as a multiclass classification problem and use [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Think about how the network's output has to change.\n",
    "  * When changing architectures substantially, you may find that the same optimization hyperparameters no longer work.\n",
    "  * pytorch RNN modules return two outputs, if you want to use attention, you want the first output (cell values at the top layer at all timesteps), for vanilla seq2seq or classification you need the second output (hidden states at all layers at the final timestep)\n",
    "\n",
    "*80 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vj-f-mHWoey"
   },
   "source": [
    "##C.3: Model compression via Weight Matrix Decomposition\n",
    "\n",
    "Model compression is the generic name used to refer to techniques applied to model architectures so that they can be more system resource efficient -- especially at inference time. (If you wish to read more about this area feel free to consult this [tutorial](https://arxiv.org/abs/1703.09039), but note it is not needed in this assignment.) In this mini-project, you will have the opportunity to explore one of these methods that involves decomposing a single large weight matrix of an architecture into a series of smaller matrices to reduce memory and compute needs, while still maintaining accuracy levels of the original model. \n",
    "\n",
    "A popular early approach of this type involved using matrix factorization methods (like SVD) and applying them to layers within an architecture where the weight matrix causes performance bottlenecks for the whole model. In this case a single weight matrix is broken down into two smaller matrices, the product of which approximates the original larger matrix.\n",
    "\n",
    "This mini-project is comprised by the following following steps and questions below. We ask that you work with a pretrained [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) and the ImageNet dataset. We will be using SVD to perform the matrix factorization:\n",
    "\n",
    "* Applying SVD decomposition to AlexNet's fully-connected (dense) layers *(~20 marks).*\n",
    "  * Perform SVD on the weight matrix of the first fully-connected layer of AlexNet. The weight matrix of the layer will be decomposed into two matrices ($U$ and $V$ by SVD) *(note, feel free to use a library implementation of SVD)*.\n",
    "  * Under which conditions will the use of the $U$ and $V$ matrices above, when used in place of the weight matrix, result in parameter savings? \n",
    "  * Implement a new layer type that you can use to replace the fully-connected layers within AlexNet. This new layer type should replace the single weight matrix in a conventional fully-connected layer with the $U$ and $V$ matrices. Include a parameter in your layer that determines how many singular vectors are used (parameter $k$ within SVD) *(note, you only need to implement the forward-pass of this layer given how we use it next)*.\n",
    "  * Replace both fully-connected layers within AlexNet with your new layer type developed above. Experiment by varying the values of $k$ and see how this changes the test accuracy of the model *(note, to speed-up experiments only compute accuracy for a 128-example subset of ImageNet test data, also only use $k$ values that result in an overal reduction of parameters in the model)*.\n",
    "* Implementing the SVD decomposition as two seperate layers *(~40 marks).*\n",
    "  * Show that the new layer you implemented above, to support the decomposition of weights, can also be viewed (and implemented) as two seperate layers. Provide an illustration to show this is possible, and also provide a new implementation of a layer type to support this *(note, again at this stage only the forward pass of the implementation is required)*.\n",
    "  * Describe the implications to the per-layer working set memory at inference time (defined during lectures) of implementing weight decomposition as two layers compare to when it is implemented as a single layer? \n",
    "* Re-training your SVD decompositions *(~20 marks).*\n",
    "  * Implement the backwards pass of the two-layer approach to supporting SVD-based weight decomposition.\n",
    "  * Re-train the AlexNet model with both fully-connected layers using this two-layer implementation *(note, you must re-train all model parameters, not just those related to the fully-connected layers)*. Did test accuracy improve? *(please use the 128-example subset of ImageNet used earlier)*. If you observe an improvement discuss why this is the case.\n",
    "  * Comment on the similarity and design considerations of your modified version of AlexNet to the MobileNet architecture discussed in lectures.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBO_QjHveA4c"
   },
   "source": [
    "## C.4: Investigating Gradient Stabilization\n",
    "\n",
    "As Ferenc mentioned during his lectures gradient instability is a fundemental barrier to successful training. In this mini-project we will dive into this issue. \n",
    "\n",
    "Before beginning, construct a version of MobileNetV2 from which you have removed the architectural components that manage the gradient. That is remove from it batch normalization layers and residual connections. We will use this \"stripped\" version of MobileNetV2 multiple times below (we call it below \"stripped MobileNetV2\"). You will recall this architecture from the prior assignment. \n",
    "\n",
    "*As a hint: you will also find it useful to implement a helper function that computes the average gradient at each layer when provided a batch of training examples.*\n",
    "\n",
    "* Early Training Gradient Stabilization *(~20 marks).*\n",
    "  * Use the stripped MobileNetV2 described above and begin by initalizing it (rather than using any pretrained weights). Create three versions related to three initialization schemes: (1) all parameters set to a constant; (2) all set as random uniform; (3) all set using Xavier initialization (i.e., use the default PyTorch initialization). (For some background on this topic consult this [paper](https://arxiv.org/abs/1704.08863).)\n",
    "  * Visualize the gradients generated when training these three networks using one batch of data taken from ImageNet, holding everything else constant. *(note, use the visualization helper function described earlier)*. Briefly comment on the differences you see. Expand your set of experiments by also testing with at least three different constants for the first scheme described (e.g., try using: $0$, $1$, $1000$).\n",
    "* Mid-Training Gradient Stabilization *(~40 marks).*\n",
    "  * We will now investigate gradients that are in-use during *mid-training*. These gradients will be available if you take the original MobileNetV2 architecture amd train it for three epochs using ImageNet using all the settings (e.g., initalizer etc.) detailed in the original [paper](https://arxiv.org/abs/1801.04381). Copy the weights of this network to the \"stripped\" MobileNetV2.  \n",
    "  * Investigate the role played by the residual connections. Insert the residual connections back to the model, and backpropagate a single batch (ImageNet). Collect and visualize the gradients using your helper function. Comment on what are the effects of inserting the residual connections back to the early, middle, and the final layers of the network. \n",
    "  * Next, investigate the role played by the batch normalization. Insert the batch normalization layers back to the model, and backpropagate a single batch through the model. Collect and visualize the gradients using your helper function. Comment on what effects does inserting the batch normalization back to the beginning, middle, and the end of the network have. *(note, use the batch normalization in its training mode)*.\n",
    "  * Compare the effects of the residual connections and the batch normalizations.\n",
    "* Gradients - Going Beyond Training *(~20 marks).* \n",
    "  * For this final exploration, revert back to a pretrained MobileNetV2 without any changes to the architecture. In the following steps we will freeze the model's parameters, and propagate gradients into the input.\n",
    "  * Define a simple loss function that is minimized when the output activation corresponding to one ImageNet class is maximized and all other classes have 0 value. (Let's call this class $C$, you can select any ImageNet class you wish). \n",
    "  * Given this loss, now identify an input image that would minimize the loss. Perform a test where you initialize an input in four different ways: (1) all zeros; (2) random; (3) image of the class $C$; (4) image not of the class $C$. Compute the gradients of the loss function with respect to the pixels in the input image. *(note, the network parameters are held constant!)* Update the image using these gradients. Repeat gradient computation and update until image stops changing.\n",
    "  * Comment on training the image. What kind of image did you arrive at -- is it representative of the class you were training towards? How did the image change as you were \"training\" it? And, what difference did the initialization strategies make - did you arrive at different final images and if so then why? If not, then why not?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mNfY7MfGW5eh"
   ],
   "name": "05-02-assignment-2-no-answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1fc73e90e9584f869450f517def60d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3d259b0d17cf4372b743bafb54ce7558": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bca8f2e8bbb462191b0af4f84c0783d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1d7f0b1b97348ab802220d08339388d",
       "IPY_MODEL_bb63d9b71fc040a2820d60eb8a100a34"
      ],
      "layout": "IPY_MODEL_3d259b0d17cf4372b743bafb54ce7558"
     }
    },
    "a262986b65094e299f23b461dd697ef0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac8d7d91d12a438da6c0419ca8d2a52f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb63d9b71fc040a2820d60eb8a100a34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac8d7d91d12a438da6c0419ca8d2a52f",
      "placeholder": "â",
      "style": "IPY_MODEL_a262986b65094e299f23b461dd697ef0",
      "value": " 548M/548M [01:20&lt;00:00, 7.13MB/s]"
     }
    },
    "c1d7f0b1b97348ab802220d08339388d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea00f36b5945488b9ff1c07fd5df2460",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fc73e90e9584f869450f517def60d16",
      "value": 574673361
     }
    },
    "ea00f36b5945488b9ff1c07fd5df2460": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
